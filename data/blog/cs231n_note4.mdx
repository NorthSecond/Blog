---
date: '2022-08-13 18:00:00 +0800'
tags: [CS231n, Deep Learning, Notes]
title: CS231n笔记-第四部分
---

## 目录

- [反向传播](#反向传播)
  - [简介](#简介)
      - [目标](#目标)
      - [问题陈述](#问题陈述)
      - [动机](#动机)
  - [梯度的简单表达式和解释](#梯度的简单表达式和解释)
  - [复合表达式，链式法则和反向传播](#复合表达式链式法则和反向传播)
  - [反向传播的直观理解](#反向传播的直观理解)
  - [模块：Sigmoid 例子](#模块sigmoid-例子)
      - [实现提示：分段反向传播](#实现提示分段反向传播)
  - [反向传播实践：分段计算](#反向传播实践分段计算)
  - [回传流中的模式](#回传流中的模式)
  - [用向量化操作梯度](#用向量化操作梯度)
      - [使用小而具体的例子](#使用小而具体的例子)
  - [小结](#小结)
  - [参考文献](#参考文献)

# 反向传播

## 简介

#### 目标

本节内容帮助读者对 **反向传播** 形成一个直观而专业的理解。反向传播是一种使用 **链式法则** 计算表达式的梯度的方法。理解这个过程和其精妙过程将会对于理解并高效实现、设计和调试神经网络至关重要。

#### 问题陈述

本节的核心问题是：给定一个函数 $f(x)$，其中 $x$ 是输入数据的向量，我们需要计算的是函数 $f$ 关于 $x$ 的梯度，也就是 $\nabla f(x)$。

#### 动机

之所以关注上述这个问题，是因为在神经网络中，有损失函数 $L$ 作为上面问题中的损失函数 $f$，输入的 $x$ 中包含训练数据和神经网络的权重。举个例子，损失函数可以是 SVM 对应的损失函数，而输入向量则包括了训练数据 $(x_i,y_i),\quad i = 1\ldots N$ 和权重偏差 $W$ 和 $b$。注意到我们认为训练集是给定的（在机器学习中通常都是这样），而权重则是可以控制的变量。因此，即使能用反向传播计算输入数据 $x_i$ 上的梯度，但是在实践中为了进行参数更新，我们通常也只计算参数（例如 $W，b$ 的梯度）。然而 $x_i$ 的梯度有时候仍然是有用的：比如将神经网络所做的事情可视化以便于我们直观地进行理解的时候，就能用上 $x_i$ 的梯度。

即使你之前对于使用链式法则计算偏微分已经很熟练，我们仍然建议你浏览这篇笔记。一维这篇笔记呈现了一个相对成熟的反向传播的视角，在这个视角中我们可以看见一个基于实数回路的反向传播过程，而它对于细节的理解和收获也能够帮助读者更好地全面学习这个课程。

## 梯度的简单表达式和解释

从简单的表达式入手有助于我们为后面的复杂表达式打好符号和规则基础。先考虑一个简单的二元乘法函数 $f(x,y) = x,y$。对于这个函数，对两个输入变量分别求偏导还是蛮简单的：

$$
f(x,y) = x y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = y \hspace{0.5in} \frac{\partial f}{\partial y} = x 
$$

**解释**：时刻牢记导数的意义：导数表示函数在某个点周围无穷小的区域内的变化变化率：

$$
\frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}
$$

一个技术说明是：等号两侧的分号所表达的意义有所不同。左边的分号并不代表除法或者是分数，而是表示操作符 $\frac{d}{dx}$ 正在被应用与函数 $f$,并且返回一个和原函数不同的函数(即导函数)。对于上面的式子一个好的考虑方法是，当 $h$ 非常小的时候，函数可以被一条直线所近似，而导数就对应着这条直线的斜率。换句话说，每个变量的导数都指明了这个被求导的变量的敏感程度。例如，若 $x = 4,y = -3$，则有 $f(x,y) = -12$，$\frac{\partial f}{\partial x} = -3$。这说明如果变量 $x$ 的值稍微变大一点点，那么整个函数值就会减小（因为导数为负），而且减小量是 $x$ 增大量的三倍。通过重新排列公式（$f(x + h) = f(x) + h\frac{df(x)}{dx}$）我们也能看到这一点。同理，由于 $\frac{\partial f}{\partial y} = 4$，可以知道如果将 $y$ 的值增加 $h$，那么函数对应的输出也会增加 $4h$。

> 函数关于每个变量的导数指明了整个函数对于该变量的敏感程度。

我们也可以对加法运算进行求导操作：

$$
f(x,y) = x + y \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = 1 \hspace{0.5in} \frac{\partial f}{\partial y} = 1
$$

这就是说，无论其值如何，$x,y$的导数均为1。这是有道理的，因为无论增加中哪一个自变量的值，函数的值都会增加，并且增加的变化率独立于 $x$ 和 $y$ 的具体值（情况和乘法操作不同）。取*最大值*操作也是常常使用的：

$$
f(x,y) = \max(x, y) \hspace{0.5in} \rightarrow \hspace{0.5in} \frac{\partial f}{\partial x} = \mathbb{1}(x >= y) \hspace{0.5in} \frac{\partial f}{\partial y} = \mathbb{1}(y >= x)
$$

上式是说，如果该变量比另一个变量大，那么梯度是1，反之为0。例如，若 $x = 4,y = 2$，那么max是4，所以函数对于 $y$ 就不敏感。也就是说，在 $y$ 上增加 $h$，函数还是输出为4，所以梯度是0：因为对于函数输出是没有效果的。当然，如果给 $y$ 增加一个很大的量，比如大于2，那么函数的值就变化了，但是导数并没有指明输入量有很大的变化情况下对于函数的效果，他们只适用于输入量变化极小时的情况，因为导数的定义就已经指明了我们讨论的是 $\lim_{h \to 0}$ 的情况。

## 复合表达式，链式法则和反向传播

现在考虑更复杂的都偶按函数，比如 $f(x,y,z) = (x + y)z$。虽然这个表达非常简单而且可以直接微分求导。但是我们在这里使用一种有助于读者直观地理解反向传播方法背后的原理。注意到这个函数的表达式可以分为两个表达式： $q = x + y,\text{and} f = qz$。在前面我们已经介绍过这两种函数对应的计算导数的方法，即因为 $f$ 是 $q$ 和 $z$ 相乘，所以有 $\frac{\partial f}{\partial q} = z$，$\frac{\partial f}{\partial z} = q$，又由于 $q = x + y$，所以有 $\frac{\partial q}{\partial x} = 1$，$\frac{\partial q}{\partial y} = 1$。然而我们实际上并不需要指导中间量 $q$ 的梯度，因为在参数更新的过程中并不涉及 $\frac{\partial f}{\partial q}$，而涉及函数 $f$ 关于自变量 $x,y,z$ 的梯度。**链式法则**指出了将梯度表达式连接起来的正确算子是乘法，比如有 $\frac{\partial f}{\partial x} = \frac{\partial f}{\partial q} \frac{\partial q}{\partial x}$。在实践操作中，计算就简化为了将两个梯度数值相乘即可。示例代码如下：

```python
# set some inputs  
x = -2; y = 5; z = -4  
  
# perform the forward pass  
q = x + y # q becomes 3  
f = q * z # f becomes -12  
  
# perform the backward pass (backpropagation) in reverse order:  
# first backprop through f = q * z  
dfdz = q # df/dz = q, so gradient on z becomes 3  
dfdq = z # df/dq = z, so gradient on q becomes -4  
dqdx = 1.0  
dqdy = 1.0  
# now backprop through q = x + y  
dfdx = dfdq * dqdx  # The multiplication here is the chain rule!  
dfdy = dfdq * dqdy    
```

最后计算得到变量的梯度 `[dfdx, dfdy, dfdz]`，代表着函数 `f` 对于变量 `[x, y, z]` 的敏感程度。这是一个最简单的反向传播。我们一半会使用一个更简介的符号在代码中表示对应的梯度，这样就不用写 `df` 了。这就是说我们可以用 `dq` 来代替 `dfdq`，且总是假设梯度是关于最终的输出的。

这个计算可以被用下面的图像可视化：

<div class="fig figleft fighighlight">
<svg style="max-width: 420px" viewbox="0 0 420 220"><defs><marker id="arrowhead" refX="6" refY="2" markerWidth="6" markerHeight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><line x1="40" y1="30" x2="110" y2="30" stroke="black" stroke-width="1"></line><text x="45" y="24" font-size="16" fill="green">-2</text><text x="45" y="47" font-size="16" fill="red">-4</text><text x="35" y="24" font-size="16" text-anchor="end" fill="black">x</text><line x1="40" y1="100" x2="110" y2="100" stroke="black" stroke-width="1"></line><text x="45" y="94" font-size="16" fill="green">5</text><text x="45" y="117" font-size="16" fill="red">-4</text><text x="35" y="94" font-size="16" text-anchor="end" fill="black">y</text><line x1="40" y1="170" x2="110" y2="170" stroke="black" stroke-width="1"></line><text x="45" y="164" font-size="16" fill="green">-4</text><text x="45" y="187" font-size="16" fill="red">3</text><text x="35" y="164" font-size="16" text-anchor="end" fill="black">z</text><line x1="210" y1="65" x2="280" y2="65" stroke="black" stroke-width="1"></line><text x="215" y="59" font-size="16" fill="green">3</text><text x="215" y="82" font-size="16" fill="red">-4</text><text x="205" y="59" font-size="16" text-anchor="end" fill="black">q</text><circle cx="170" cy="65" fill="white" stroke="black" stroke-width="1" r="20"></circle><text x="170" y="70" font-size="20" fill="black" text-anchor="middle">+</text><line x1="110" y1="30" x2="150" y2="65" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="110" y1="100" x2="150" y2="65" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="190" y1="65" x2="210" y2="65" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="380" y1="117" x2="450" y2="117" stroke="black" stroke-width="1"></line><text x="385" y="111" font-size="16" fill="green">-12</text><text x="385" y="134" font-size="16" fill="red">1</text><text x="375" y="111" font-size="16" text-anchor="end" fill="black">f</text><circle cx="340" cy="117" fill="white" stroke="black" stroke-width="1" r="20"></circle><text x="340" y="127" font-size="20" fill="black" text-anchor="middle">*</text><line x1="280" y1="65" x2="320" y2="117" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="110" y1="170" x2="320" y2="117" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="360" y1="117" x2="380" y2="117" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line></svg>
</div>

上图中的真实值计算线路展示了视觉化的计算过程。在**前向传播**过程中，我们从输入向输出计算（绿色数字所示），而在**后向传播**过程中，我们从尾部开始，根据链式法则向前计算梯度，一直到网络的输入端（红色数字所示）。可以认为，梯度是计算链路中的一种回流。

---

## 反向传播的直观理解

注意到反向传播是一个很优美的局部过程。在整个计算线路中，每一个门单元都会得到一些输入并且会立即计算两个东西：1.这个门的输出值；2.其输出值关于输入值的局部梯度。门单元完成这两件事情的过程是完全独立的，不涉及计算线路中的其他细节。然而，一旦前向传播结束之后，在反向传播的过程中门单元将会最终获得整个网络的最终输出值关于自己的输出值上的梯度。链式法则指出，门单元应该将回传的梯度乘以它对其的输入的局部梯度，从而得到整个网络的输出对于该门单元的每个输入值的梯度。

>这里关于每一个输出的乘法操作都是基于链式法则的。这个操作可以让一个相对独立简单的门单元成为复杂计算线路中不可或缺的一部分，这个复杂计算线路可以是神经网络等。

让我们通过下面这个例子来对这一过程进行理解。加阀门收到了输入 [-2, 5]，其计算得到的输出值是3。既然这个门是加法门，那么其对应的对于两个变量的局部梯度都是 +1。网络的其余部分计算出最终值为-12。在反向传播时将递归地使用链式法则，算到加法门（作为乘法门的输入）的时候，知道加法门的输出的梯度是-4。如果网络如果想要输出值更高，那么可以认为它会想要加法门的输出更小一点（因为负号），而且还有一个4的倍数。继续递归并对梯度使用链式法则，加法门拿到梯度，然后把这个梯度分别乘到每个输入值的局部梯度（就是让-4乘以**x**和**y**的局部梯度，x和y的局部梯度都是1，所以最终都是-4）。可以看到我们得到了想要的效果：如果**x，y减小**（它们的梯度为负），那么加法门的输出值减小，这会让乘法门的输出值增大。

因此，反向传播可以看作是门单元之间通过梯度信号相互进行通信，只要让它们的输入沿着梯度方向变化，无论它们自己的输出值在何种程度上上升或降低，都是为了让整个网络的输出值变得更高。

## 模块：Sigmoid 例子

上面介绍的门的概念其实是相对比较随意的。任何可以微分的函数都可以看作是一个门。当然我们看可以将多个门组合成为一个门，也可以根据需要将一个门拆分成为多个门。现在看下述表达式：

$$
f(\omega,x) = \frac{1}{1 + e^{-(\omega_0x_0 + \omega_1x_1+\omega_2)}}
$$

在本课程后面的部分中我们可以看到，这个表达式事实上描述了一个二维神经元（包含输入 $x$ 和权重 $\omega$），该神经元使用了 *sigmoid* *激活函数（activation）*。但是在现在来看，我们只是做了一个简单的输入为 $x$ 和 $\omega$ 输出为一个数字的函数。这个函数由多个门组成。除了上文介绍的加法门，乘法门，最大值门，还有下面这四种：

$$
f(x) = \frac{1}{x} 
\hspace{1in} \rightarrow \hspace{1in} 
\frac{df}{dx} = -1/x^2 
\\\\
f_c(x) = c + x
\hspace{1in} \rightarrow \hspace{1in} 
\frac{df}{dx} = 1 
\\\\
f(x) = e^x
\hspace{1in} \rightarrow \hspace{1in} 
\frac{df}{dx} = e^x
\\\\
f_a(x) = ax
\hspace{1in} \rightarrow \hspace{1in} 
\frac{df}{dx} = a
$$

其中，函数 $f_c$ 对输入值进行了常量 $c$ 的平移，而 $f_a$ 将输入值扩大为了常量的a倍。它们是加法和乘法的特例，但这里我们将其看作一元的门单元，因为我们确实需要计算常量 $c$ 和 $a$ 的梯度。整个计算线路如下：

使用 sigmoid 激活函数的二维神经元的例子。这里输入是 $[x_0,x_1]$，可学习的权重是$[w_0,w_1,w_2]$。正如我们稍后可以看到的，整个神经元对输入数据做点积运算，然后其激活数据被 Sigmoid 函数挤压到 0 到 1 之间。

---

在上面的例子中我们可以看见一个函数操作的全链过程，链条上的门都对 $w$ 和 $x$ 的点积结果进行操作。这个函数被称为 sigmoid 函数 $\sigma(x)$。sigmoid 函数关于其输入的求导是可以简化的(使用了在分子上先加后减1的技巧)：

$$
\sigma(x) = \frac{1}{1+e^{-x}} \\\\
\rightarrow \hspace{0.3in} \frac{d\sigma(x)}{dx} = \frac{e^{-x}}{(1+e^{-x})^2} = \left( \frac{1 + e^{-x} - 1}{1 + e^{-x}} \right) \left( \frac{1}{1+e^{-x}} \right) 
= \left( 1 - \sigma(x) \right) \sigma(x)
$$

正如我们所看到的一样，梯度计算变得简单了很多。举个例子，sigmoid 输入为1.0，则在前向传播中计算出输出为0.73。根据上面的公式，局部梯度为 $ (1 - 0.73) \times 0.73 \approx 0.2$，和之前的计算流程比起来，现在的计算只需要使用一个单独的简单表达式。因此，在实际的应用中将这些操作装进一个单独的门单元中将会非常有用。该神经元反向传播的代码实现如下：

```python
w = [2,-3,-3] # assume some random weights and data
x = [-1, -2]

# forward pass
dot = w[0]*x[0] + w[1]*x[1] + w[2]
f = 1.0 / (1 + math.exp(-dot)) # sigmoid function

# backward pass through the neuron (backpropagation)
ddot = (1 - f) * f # gradient on dot variable, using the sigmoid gradient derivation
dx = [w[0] * ddot, w[1] * ddot] # backprop into x
dw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # backprop into w
# we're done! we have the gradients on the inputs to the circuit
```

#### 实现提示：分段反向传播

上面的代码展示了在实际操作中，为了使反向传播过程更加简洁，把传播分成不同的阶段将是一种有效的方法。比如我们创建了一个中间变量 `dot`，储存 `w` 和 `x` 的点乘结果。在反向传播的时，就可以（反向地）计算出装着 `w` 和 `x` 等的梯度的对应的变量（比如 `ddot`，`dx `和 `dw`）。

本节的要点就是展示反向传播的细节过程，以及前向传播过程中，哪些函数可以被组合成门，从而可以进行简化。知道表达式中哪部分的局部梯度计算比较简洁非常有用，这样他们可以“链”在一起，让代码量更少，效率更高。

## 反向传播实践：分段计算

让我们看另一个例子。假设由如下函数：

$$
f(x,y) = \frac{x + \sigma(y)}{\sigma(x) + (x+y)^2}
$$

首先需要明确的是，这个函数完全没用，在神经网络中是不会用到它来进行梯度计算的，我们在这里只是将它作为一个实现反向传播的一个例子。需要强调的是，如果对 $x$ 或者 $y$ 进行微分运算，运算的结果将会是一个非常巨大而复杂的表达式。然而，事实证明这样复杂的计算是完全没有必要的，因为我们其实并不需要一个明确的显式的梯度函数表达式。我们只需要知道如何使用反向传播计算梯度即可。下联是一个构建前向传播的代码模板。

```python
x = 3 # example values
y = -4

# forward pass
sigy = 1.0 / (1 + math.exp(-y)) # sigmoid in numerator   		#(1)  
num = x + sigy # numerator                               		#(2)  
sigx = 1.0 / (1 + math.exp(-x)) # sigmoid in denominator 		#(3)  
xpy = x + y                                              		#(4)  
xpysqr = xpy**2                                          		#(5)  
den = sigx + xpysqr # denominator                        		#(6)  
invden = 1.0 / den                                       		#(7)  
f = num * invden # done!                                 		#(8)  
```

Phew，到表达式的最后我们就完成了前向传播。注意我们在构建代码的过程中创建了多个中间变量，其中每一个都是比较简单的表达式，它们计算局部梯度的方法都是已知的。这样计算反向传播就显得非常简单了：我们只需要对前向传播过程中产生的每一个变量（`sigy, num, sigx, xpy, xpysqr, den, invden`）进行回传。我们将会拥有相同数量的变量，但都以 `d` 开头，用来存储对应变量的梯度。注意在反向传播的每一小块内容中都包含了表达式的局部梯度，然后我们根据链式法则来计算上游的梯度。对于每行代码，我们注明其对应前向传播的哪一部分。

```python
# backprop f = num * invden  
dnum = invden # gradient on numerator                             #(8)  
dinvden = num                                                     #(8)  
# backprop invden = 1.0 / den   
dden = (-1.0 / (den**2)) * dinvden                                #(7)  
# backprop den = sigx + xpysqr  
dsigx = (1) * dden                                                #(6)  
dxpysqr = (1) * dden                                              #(6)  
# backprop xpysqr = xpy**2  
dxpy = (2 * xpy) * dxpysqr                                        #(5)  
# backprop xpy = x + y  
dx = (1) * dxpy                                                   #(4)  
dy = (1) * dxpy                                                   #(4)  
# backprop sigx = 1.0 / (1 + math.exp(-x))  
dx += ((1 - sigx) * sigx) * dsigx # Notice += !! See notes below  #(3)  
# backprop num = x + sigy  
dx += (1) * dnum                                                  #(2)  
dsigy = (1) * dnum                                                #(2)  
# backprop sigy = 1.0 / (1 + math.exp(-y))  
dy += ((1 - sigy) * sigy) * dsigy                                 #(1)  
# done! phew  
```

需要注意以下几点：

**对前向传播过程中的变量进行缓存**：在反向传播的计算过程中，前向传播过程中的一些中间变量是非常有用的。在实际的操作过程中，我们最好实现代码完成对于这些中间变量的缓存，这样在计算反向传播的过程中也可以用上它们。如果这样做存在困难，也可以在反向传播时重新完成计算（但是会浪费计算资源）。

**叠加不同分支上的梯度**：如果变量x，y在前向传播的表达式中出现多次，那么进行反向传播的时候就要非常小心，使用**+=**而不是**=**来累计这些变量的梯度（不然就会造成覆写）。这是遵循了在微积分中的*多元链式法则*，该法则指出如果变量在线路中分支走向不同的部分，那么梯度在回传的时候，就应进行累加。

## 回传流中的模式

一个有趣的现象时，在许多情况下，反向传播过程中的梯度可以被很直观地进行解释。例如神经网络中最常用的加法、乘法和取最大值这三个门单元，它们在反向传播过程中的行为都有非常简单的解释。先看下面这个例子：

<div class="fig figleft fighighlight">
<svg style="max-width: 460px" viewbox="0 0 460 290"><g transform="scale(1)"><defs><marker id="arrowhead" refX="6" refY="2" markerWidth="6" markerHeight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><line x1="50" y1="30" x2="90" y2="30" stroke="black" stroke-width="1"></line><text x="55" y="24" font-size="16" fill="green">3.00</text><text x="55" y="47" font-size="16" fill="red">-8.00</text><text x="45" y="24" font-size="16" text-anchor="end" fill="black">x</text><line x1="50" y1="100" x2="90" y2="100" stroke="black" stroke-width="1"></line><text x="55" y="94" font-size="16" fill="green">-4.00</text><text x="55" y="117" font-size="16" fill="red">6.00</text><text x="45" y="94" font-size="16" text-anchor="end" fill="black">y</text><line x1="50" y1="170" x2="90" y2="170" stroke="black" stroke-width="1"></line><text x="55" y="164" font-size="16" fill="green">2.00</text><text x="55" y="187" font-size="16" fill="red">2.00</text><text x="45" y="164" font-size="16" text-anchor="end" fill="black">z</text><line x1="50" y1="240" x2="90" y2="240" stroke="black" stroke-width="1"></line><text x="55" y="234" font-size="16" fill="green">-1.00</text><text x="55" y="257" font-size="16" fill="red">0.00</text><text x="45" y="234" font-size="16" text-anchor="end" fill="black">w</text><line x1="170" y1="65" x2="210" y2="65" stroke="black" stroke-width="1"></line><text x="175" y="59" font-size="16" fill="green">-12.00</text><text x="175" y="82" font-size="16" fill="red">2.00</text><circle cx="130" cy="65" fill="white" stroke="black" stroke-width="1" r="20"></circle><text x="130" y="75" font-size="20" fill="black" text-anchor="middle">*</text><line x1="90" y1="30" x2="110" y2="65" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="90" y1="100" x2="110" y2="65" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="150" y1="65" x2="170" y2="65" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="170" y1="205" x2="210" y2="205" stroke="black" stroke-width="1"></line><text x="175" y="199" font-size="16" fill="green">2.00</text><text x="175" y="222" font-size="16" fill="red">2.00</text><circle cx="130" cy="205" fill="white" stroke="black" stroke-width="1" r="20"></circle><text x="130" y="210" font-size="20" fill="black" text-anchor="middle">max</text><line x1="90" y1="170" x2="110" y2="205" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="90" y1="240" x2="110" y2="205" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="150" y1="205" x2="170" y2="205" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="290" y1="135" x2="330" y2="135" stroke="black" stroke-width="1"></line><text x="295" y="129" font-size="16" fill="green">-10.00</text><text x="295" y="152" font-size="16" fill="red">2.00</text><circle cx="250" cy="135" fill="white" stroke="black" stroke-width="1" r="20"></circle><text x="250" y="140" font-size="20" fill="black" text-anchor="middle">+</text><line x1="210" y1="65" x2="230" y2="135" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="210" y1="205" x2="230" y2="135" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="270" y1="135" x2="290" y2="135" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="410" y1="135" x2="450" y2="135" stroke="black" stroke-width="1"></line><text x="415" y="129" font-size="16" fill="green">-20.00</text><text x="415" y="152" font-size="16" fill="red">1.00</text><circle cx="370" cy="135" fill="white" stroke="black" stroke-width="1" r="20"></circle><text x="370" y="140" font-size="20" fill="black" text-anchor="middle">*2</text><line x1="330" y1="135" x2="350" y2="135" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line><line x1="390" y1="135" x2="410" y2="135" stroke="black" stroke-width="1" marker-end="url(#arrowhead)"></line></g></svg>
</div>

这是一个展示反向传播的例子。加法操作将梯度相等地分发给它的输入。而取最大操作将梯度路由给更大的输入。乘法门拿取输入激活数据，并对他们进行交换，然后乘以梯度。

---

以上图为例，我们可以看到：

**加法门单元**把输出的梯度相等地分发给它所有的输入，这一行为与输入值在前向传播时的值无关。这是因为加法操作的局部梯度都是简单的+1，所以所有输入的梯度实际上就等于输出的梯度，因为乘以1.0保持不变。上例中，加法门把梯度2.00不变且相等地路由给了两个输入。

**取最大值门单元**对梯度做路由。和加法门不同，取最大值门将梯度转给其中一个输入，这个输入是在前向传播中值最大的那个输入。这是因为在取最大值门中，最高值的局部梯度是1.0，其余的是0。上例中，取最大值门将梯度2.00转给了**z**变量，因为**z**的值比**w**高，于是**w**的梯度保持为0。

**乘法门单元**相对不容易解释。它的局部梯度就是输入值，但是是相互交换之后的，然后根据链式法则乘以输出值的梯度。上例中，**x**的梯度是 $-4.00\times 2.00 = -8.00$。

*非直观影响及其结果*。我们要注意一种特殊情况，如果乘法门单元的其中一个输入非常小，但另一个输入非常大，那么乘法门的操作将不会那么直观：它将会把大的梯度分配给小的输入，把小的梯度分配给大的输入。在线性分类器中，权重和输入是进行点积 $w^Tx_i$，这说明输入数据的大小对于权重梯度的大小有影响。例如，在计算过程中对所有输入数据样本 $x_i$ 乘以1000，那么权重的梯度将会增大1000倍，这样就必须降低学习率来弥补。这就是为什么数据预处理关系重大，它即使只是有微小变化，也会产生巨大影响。对于梯度在计算线路中是如何流动的有一个直观的理解，可以帮助读者调试网络。

## 用向量化操作梯度

上述内容考虑的都是单个变量情况，但是所有概念都适用于矩阵和向量操作。然而，在操作的时候要注意关注维度和转置操作。

**矩阵相乘的梯度**：可能最有技巧的操作是矩阵相乘（也适用于矩阵和向量，向量和向量相乘）的乘法操作：

```python
# forward pass  
W = np.random.randn(5, 10)  
X = np.random.randn(10, 3)  
D = W.dot(X)  
  
# now suppose we had the gradient on D from above in the circuit  
dD = np.random.randn(*D.shape) # same shape as D  
dW = dD.dot(X.T) #.T gives the transpose of the matrix  
dX = W.T.dot(dD)  
```

*提示：要分析维度！* 注意我们不需要去记忆 `dW` 和 `dX` 的表达，因为它们很容易通过维度推导出来。例如，权重的梯度 `dW` 的尺寸肯定和权重矩阵 `W` 的尺寸是一样的，而这又是由 `X` 和 `dD` 的矩阵乘法决定的（在上面的例子中 `X` 和 `W` 都是数字不是矩阵）。总有一个方式是能够让维度之间能够对的上的。例如， `X` 的尺寸是$[10\times 3]$，`dD` 的尺寸是 $[5\times 3]$，如果你想要 `dW` 和 `W` 的尺寸是 $[5\times 10]$，那就要执行操作 `dD.dot(X.T)` 。

#### 使用小而具体的例子

有些读者可能觉得向量化操作的梯度计算比较困难，建议是写出一个很小很明确的向量化例子，在纸上演算梯度，然后对其一般化，得到一个高效的向量化操作形式。

Erik Learned-Miller 还撰写了一篇较长的相关文档，介绍了矩阵/向量导数，它可能会有帮助。[在这里找到它](http://cs231n.stanford.edu/vecDerivs.pdf)。

## 小结

在本章中，我们：

* 对梯度的含义有了直观理解，知道了梯度是如何在网络中反向传播的，知道了它们是如何与网络的不同部分通信并控制其升高或者降低，并使得最终输出值更高的。
* 讨论了**分段计算**在反向传播的实现中的重要性。应该将函数分成不同的模块，这样计算局部梯度相对容易，然后基于链式法则将其“链”起来。重要的是，不需要把这些表达式写在纸上然后演算它的完整求导公式，因为实际上并不需要关于输入变量的梯度的数学公式。只需要将表达式分成不同的可以求导的模块（模块可以是矩阵向量的乘法操作，或者取最大值操作，或者加法操作等），然后在反向传播中一步一步地计算梯度。

在下节课中，将会开始定义神经网络，而反向传播使我们能高效计算神经网络各个节点关于损失函数的梯度。换句话说，我们现在已经准备好训练神经网络了，本课程最困难的部分已经过去了！ConvNets相比只是向前走了一小步。

## 参考文献

- [Automatic differentiation in machine learning: a survey](http://arxiv.org/abs/1502.05767)